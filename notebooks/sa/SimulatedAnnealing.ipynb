{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random as rm\n",
    "\n",
    "# Add the sibling folders\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "import src.utils as ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a basic NN, made out of two layers: the first one has 100 neurons, while the second one 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Define all the components\n",
    "        # Basic two-layer network\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        #self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the acutal network\n",
    "        \n",
    "        # Get the batch size\n",
    "        in_size = x.size(0)\n",
    "        print(x.size())\n",
    "        # Flatten data, -1 is inferred from the other dimensions\n",
    "        x = x.view(in_size, -1) \n",
    "        \n",
    "        # Forward rule\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "The first implementation uses the whole training set, in order to avoid noisy evaluations. In such a way we are sure to make the best choice. \n",
    "The epsilon variable allows to set the maximum movement in each direction (similar to learning rate). Each movement is done by creating a tensor for each layer called $ \\Delta(w) $.\n",
    "\n",
    "$$ w'  = w - \\epsilon \\Delta(w)  $$\n",
    "\n",
    "where each value of the tensor  $ \\Delta(w) $ is sampled from the distribution $ U (-1, 1) $. \n",
    "\n",
    "In the following implementation, if $ L(w') > L(w) $ the opposite direction is chosen.\n",
    "\n",
    "$$ w''  = w + \\epsilon \\ \\Delta(w)  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilon = 10e-2\n",
    "net = Net().cuda()\n",
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=4096)\n",
    "        \n",
    "def full_train_SA(trainloader, model, accuracy_before, gpu=True):\n",
    "    model.train()\n",
    "    \n",
    "    if accuracy is None:\n",
    "        accuracy_before = ut.test_train(train_loader, net)\n",
    "    \n",
    "    inverse = []\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        # Replicate the tensor\n",
    "        tensor_size = param.data.size()\n",
    "        move = torch.zeros(tensor_size)\n",
    "        # Send it to the GPU\n",
    "        if gpu:\n",
    "            move = move.cuda()\n",
    "        # Generate move\n",
    "        move = move.uniform_(-1, 1).mul(epsilon) * param.data\n",
    "        # Stepback is saved\n",
    "        inverse.append(move.mul(-2))\n",
    "        # Move the parameters\n",
    "        param.data.add_(move)\n",
    "    \n",
    "    # Evaluate the accuracy \n",
    "    new_accuracy = ut.test_train(train_loader, net)\n",
    "\n",
    "    if new_accuracy[1] < accuracy_before[1]:\n",
    "        print(\"Wrong direction, in fact:\", new_accuracy[1])\n",
    "        for k, param in enumerate(net.parameters()):\n",
    "            param.data.add_(inverse[k])\n",
    "\n",
    "        new_accuracy = ut.test_train(train_loader, net)\n",
    "    \n",
    "    print(\"New accuracy: \", new_accuracy)\n",
    "    \n",
    "    return new_accuracy\n",
    "\n",
    "accuracy = None\n",
    "for epoch in range(1000):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    accuracy = full_train_SA(train_loader, net, accuracy)\n",
    "    print(\"Validation test:\", ut.test(test_loader, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, it can be seen that both the directions do not sometimes improve the accuracy. This technique allows to avoid overwriting the values if $ L(w'') > L(w') $, but in the meanwhile it makes the worst choice.\n",
    "\n",
    "The update rule can be changed to choose only the best movement. If both of them worsen the solution, the best one is picked according to the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'current_accuracy = ut.test_train(train_loader, net)[1]\\nfor epoch in range(1000):\\n    print(\"Epoch: \", epoch)\\n    current_accuracy = full_train_SA_best_step(train_loader, net, current_accuracy)\\n    print(\"Validation test:\", ut.test(test_loader, net))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 10e-2 / 2\n",
    "net = Net().cuda()\n",
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=4096)\n",
    "        \n",
    "def full_train_SA_best_step(trainloader, model, accuracy_before, gpu=True):\n",
    "    model.train()\n",
    "    \n",
    "    inverse = []\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        # Replicate the tensor\n",
    "        tensor_size = param.data.size()\n",
    "        move = torch.zeros(tensor_size)\n",
    "        # Send it to the GPU\n",
    "        if gpu:\n",
    "            move = move.cuda()\n",
    "        # Generate move\n",
    "        move = move.uniform_(-1, 1).mul(epsilon) * param.data\n",
    "        # Stepback is saved\n",
    "        inverse.append(move.mul(-2))\n",
    "        # Move the parameters\n",
    "        param.data.add_(move)\n",
    "    \n",
    "    # Evaluate the accuracy \n",
    "    first_accuracy = ut.test_train(train_loader, net)[1]\n",
    "    \n",
    "    if first_accuracy < accuracy_before:\n",
    "        for k, param in enumerate(net.parameters()):\n",
    "            param.data.add_(inverse[k])\n",
    "\n",
    "        second_accuracy = ut.test_train(train_loader, net)[1]\n",
    "        \n",
    "        if second_accuracy < first_accuracy: # Get back to the first solution\n",
    "            for k, param in enumerate(net.parameters()):\n",
    "                param.data.sub_(inverse[k])\n",
    "            new_accuracy = first_accuracy\n",
    "        else:\n",
    "            new_accuracy = second_accuracy\n",
    "    else:\n",
    "        new_accuracy = first_accuracy\n",
    "        \n",
    "    print(\"New accuracy: \", new_accuracy)\n",
    "    \n",
    "    return new_accuracy\n",
    "\n",
    "'''current_accuracy = ut.test_train(train_loader, net)[1]\n",
    "for epoch in range(1000):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    current_accuracy = full_train_SA_best_step(train_loader, net, current_accuracy)\n",
    "    print(\"Validation test:\", ut.test(test_loader, net))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10e-1 / 2\n",
    "net = Net().cuda()\n",
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=4096)\n",
    "        \n",
    "def full_train_SA_temperature(trainloader, model, accuracy_before, temperature = 0, gpu=True):\n",
    "    model.train()\n",
    "    \n",
    "    inverse = []\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        # Replicate the tensor\n",
    "        tensor_size = param.data.size()\n",
    "        move = torch.zeros(tensor_size)\n",
    "        # Send it to the GPU\n",
    "        if gpu:\n",
    "            move = move.cuda()\n",
    "        # Generate move\n",
    "        move = move.uniform_(-1, 1).mul(epsilon) * param.data\n",
    "        # Stepback is saved\n",
    "        inverse.append(move.mul(-1)) \n",
    "        # Move the parameters\n",
    "        param.data.add_(move)\n",
    "    \n",
    "    # Evaluate the accuracy \n",
    "    first_accuracy = ut.test_train(train_loader, net)[1]\n",
    "    if first_accuracy < accuracy_before:\n",
    "        for k, param in enumerate(net.parameters()):\n",
    "            param.data.add_(inverse[k].mul(2))\n",
    "\n",
    "        second_accuracy = ut.test_train(train_loader, net)[1]\n",
    "        \n",
    "        if second_accuracy < accuracy_before and temperature == 0: # Get back to the first solution\n",
    "            for k, param in enumerate(net.parameters()):\n",
    "                param.data.sub_(inverse[k])\n",
    "            new_accuracy = accuracy_before\n",
    "        else:\n",
    "            new_accuracy = second_accuracy\n",
    "    else:\n",
    "        new_accuracy = first_accuracy\n",
    "    \n",
    "    print(\"New accuracy: \", new_accuracy)\n",
    "    \n",
    "    return new_accuracy\n",
    "\n",
    "accuracy_before = ut.test_train(train_loader, net)[1]\n",
    "for epoch in range(1000):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    accuracy = full_train_SA_temperature(train_loader, net, accuracy)\n",
    "    print(\"Validation test:\", ut.test(test_loader, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with SGD \n",
    "Running the same network with the SGD algorithm to compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows to test a single minibatch, given inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_minibatch(inputs, labels, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    outputs = model(Variable(inputs))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    test_loss += float(F.cross_entropy(outputs, Variable(labels)).item())\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, optimizer, criterion, gpu=True):\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        if gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        #print(test_minibatch(inputs, labels, model))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del inputs, labels, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=512)\n",
    "net = Net().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 20):    \n",
    "    train(train_loader, net, optimizer, criterion, 1)\n",
    "    print(\"Epoch: \", epoch, \"value:\", ut.test(test_loader, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing SGD and SA\n",
    "We have already seen that the Cross entropy is a continuous approximation of the accuracy. Another approach can benefit of both the approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 value: (0.0035228755354881286, 0.7898)\n",
      "Epoch:  2 value: (0.003221932029724121, 0.8928)\n",
      "Epoch:  3 value: (0.0031670210003852846, 0.9056)\n",
      "Epoch:  4 value: (0.0031424849987030028, 0.9109)\n",
      "Epoch:  5 value: (0.003127106690406799, 0.9163)\n",
      "Epoch:  6 value: (0.0031161041021347045, 0.9194)\n",
      "Epoch:  7 value: (0.003108021366596222, 0.9227)\n",
      "Epoch:  8 value: (0.0031003273487091062, 0.926)\n",
      "Epoch:  9 value: (0.003094895625114441, 0.9279)\n",
      "Epoch:  10 value: (0.003089822828769684, 0.9287)\n",
      "Epoch:  11 value: (0.003085683298110962, 0.9304)\n",
      "Epoch:  12 value: (0.003081518316268921, 0.932)\n",
      "Epoch:  13 value: (0.0030769414067268372, 0.934)\n",
      "Epoch:  14 value: (0.0030738753199577333, 0.9354)\n",
      "Epoch:  15 value: (0.0030705927491188048, 0.9361)\n",
      "Epoch:  16 value: (0.0030673923015594484, 0.9376)\n",
      "Epoch:  17 value: (0.003064748203754425, 0.9383)\n",
      "Epoch:  18 value: (0.003061336863040924, 0.9392)\n",
      "Epoch:  19 value: (0.003059269595146179, 0.9396)\n",
      "Epoch:  20 value: (0.0030563995480537413, 0.942)\n",
      "Epoch:  21 value: (0.0030542461156845093, 0.942)\n",
      "Epoch:  22 value: (0.0030520209670066834, 0.9436)\n",
      "Epoch:  23 value: (0.0030493533849716187, 0.9444)\n",
      "Epoch:  24 value: (0.0030471731424331666, 0.9454)\n",
      "Epoch:  25 value: (0.0030447930335998534, 0.9464)\n",
      "Epoch:  26 value: (0.0030422924518585205, 0.9467)\n",
      "Epoch:  27 value: (0.0030405685544013976, 0.9474)\n",
      "Epoch:  28 value: (0.0030398016571998596, 0.9474)\n",
      "Epoch:  29 value: (0.0030372202634811403, 0.9491)\n",
      "Epoch:  30 value: (0.003035728192329407, 0.9496)\n",
      "Epoch:  31 value: (0.00303350043296814, 0.9502)\n",
      "Epoch:  32 value: (0.0030319539904594423, 0.9522)\n",
      "Epoch:  33 value: (0.0030306633949279786, 0.9519)\n",
      "Epoch:  34 value: (0.0030287709593772886, 0.9539)\n",
      "Epoch:  35 value: (0.003027968657016754, 0.9538)\n",
      "Epoch:  36 value: (0.00302632075548172, 0.9548)\n",
      "Epoch:  37 value: (0.0030248045802116394, 0.9556)\n",
      "Epoch:  38 value: (0.003023623013496399, 0.9565)\n",
      "Epoch:  39 value: (0.003022194576263428, 0.9569)\n",
      "Epoch:  40 value: (0.003021067452430725, 0.9574)\n",
      "Epoch:  41 value: (0.0030198310256004334, 0.9576)\n",
      "Epoch:  42 value: (0.003018865728378296, 0.9578)\n",
      "Epoch:  43 value: (0.0030179208993911743, 0.958)\n",
      "Epoch:  44 value: (0.003017186915874481, 0.9586)\n",
      "Epoch:  45 value: (0.003015736198425293, 0.9591)\n",
      "Epoch:  46 value: (0.0030146459221839905, 0.9598)\n",
      "Epoch:  47 value: (0.0030137740731239317, 0.9599)\n",
      "Epoch:  48 value: (0.0030132482171058655, 0.9595)\n",
      "Epoch:  49 value: (0.0030122204065322878, 0.9606)\n",
      "Epoch:  50 value: (0.0030112574696540832, 0.9606)\n",
      "Epoch:  51 value: (0.003010449802875519, 0.9613)\n",
      "Epoch:  52 value: (0.0030093193888664246, 0.9617)\n",
      "Epoch:  53 value: (0.0030091556549072264, 0.9624)\n",
      "Epoch:  54 value: (0.0030079662203788757, 0.9623)\n",
      "Epoch:  55 value: (0.003007175004482269, 0.9628)\n",
      "Epoch:  56 value: (0.0030068538188934325, 0.9626)\n",
      "Epoch:  57 value: (0.003005610418319702, 0.9638)\n",
      "Epoch:  58 value: (0.0030051199078559875, 0.9637)\n",
      "Epoch:  59 value: (0.0030041280627250673, 0.9643)\n",
      "Epoch:  60 value: (0.0030034920811653136, 0.9647)\n",
      "Epoch:  61 value: (0.0030030354619026184, 0.9645)\n",
      "Epoch:  62 value: (0.003002417540550232, 0.9652)\n",
      "Epoch:  63 value: (0.003001690649986267, 0.9651)\n",
      "Epoch:  64 value: (0.00300090469121933, 0.9656)\n",
      "Epoch:  65 value: (0.0030005229830741883, 0.9657)\n",
      "Epoch:  66 value: (0.0030002493381500244, 0.9667)\n",
      "Epoch:  67 value: (0.002999394881725311, 0.9664)\n",
      "Epoch:  68 value: (0.0029987969160079958, 0.9667)\n",
      "Epoch:  69 value: (0.002998179268836975, 0.9664)\n",
      "Epoch:  70 value: (0.002997772419452667, 0.9674)\n",
      "Epoch:  71 value: (0.0029974034667015078, 0.9677)\n",
      "Epoch:  72 value: (0.002996733868122101, 0.968)\n",
      "Epoch:  73 value: (0.0029964208126068113, 0.9677)\n",
      "Epoch:  74 value: (0.002995892083644867, 0.9675)\n",
      "Epoch:  75 value: (0.00299519761800766, 0.9686)\n",
      "Epoch:  76 value: (0.0029948070049285887, 0.9692)\n",
      "Epoch:  77 value: (0.002994361960887909, 0.9692)\n",
      "Epoch:  78 value: (0.002993833935260773, 0.9692)\n",
      "Epoch:  79 value: (0.0029935601115226745, 0.9695)\n",
      "Epoch:  80 value: (0.0029931824922561647, 0.9693)\n",
      "Epoch:  81 value: (0.0029930694580078123, 0.9696)\n",
      "Epoch:  82 value: (0.0029926114797592164, 0.9695)\n",
      "Epoch:  83 value: (0.0029919432520866393, 0.97)\n",
      "Epoch:  84 value: (0.0029916266560554503, 0.9701)\n",
      "Epoch:  85 value: (0.002990988254547119, 0.9713)\n",
      "Epoch:  86 value: (0.002990995252132416, 0.9702)\n",
      "Epoch:  87 value: (0.0029904908180236815, 0.9711)\n",
      "Epoch:  88 value: (0.0029901722192764283, 0.9707)\n",
      "Epoch:  89 value: (0.0029901803731918336, 0.9709)\n",
      "Epoch:  90 value: (0.002989859449863434, 0.9709)\n",
      "Epoch:  91 value: (0.0029894453763961793, 0.9713)\n",
      "Epoch:  92 value: (0.0029889444947242736, 0.9714)\n",
      "Epoch:  93 value: (0.0029887274146080016, 0.9716)\n",
      "Epoch:  94 value: (0.0029882997393608094, 0.9715)\n",
      "Epoch:  95 value: (0.0029883239388465883, 0.9718)\n",
      "Epoch:  96 value: (0.002988047015666962, 0.9717)\n",
      "Epoch:  97 value: (0.0029874433279037478, 0.972)\n",
      "Epoch:  98 value: (0.002987408936023712, 0.9721)\n",
      "Epoch:  99 value: (0.0029874388575553894, 0.9723)\n",
      "Epoch:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bb349c49475f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_train_SA_best_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation test:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=512)\n",
    "net = Net().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 100):    \n",
    "    train(train_loader, net, optimizer, criterion, 1)\n",
    "    print(ut.test_train(train_loader, net))\n",
    "    print(\"Epoch: \", epoch, \"value:\", ut.test(test_loader, net))\n",
    "\n",
    "accuracy = ut.test_train(train_loader, net)[1]\n",
    "for epoch in range(1000):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    accuracy = full_train_SA_best_step(train_loader, net, accuracy)\n",
    "    print(\"Validation test:\", ut.test(test_loader, net))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
