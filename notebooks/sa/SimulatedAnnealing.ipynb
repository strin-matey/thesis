{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Pytorch libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random as rm\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Add the sibling folders\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "import src.utils as ut\n",
    "\n",
    "# Plot libraries and tables\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a basic NN, made out of two layers: the first one has 200 neurons, while the second one 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Basic two-layer network\n",
    "        self.fc1 = nn.Linear(28 * 28, 10)\n",
    "        #self.fc2 = nn.Linear(200, 50)\n",
    "        #self.fc3 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # Get the batch size\n",
    "        in_size = x.size(0)\n",
    "        # Flatten data, -1 is inferred from the other dimensions\n",
    "        x = x.view(in_size, -1) \n",
    "        \n",
    "        # Forward rule\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        \n",
    "        # Softmax on predictions\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CifarNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 10, bias=True)\n",
    "        #self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet().cuda()\n",
    "for name, param in net.named_parameters():\n",
    "    print(\"Name:\", name, \" size: \", param.size())\n",
    "\n",
    "train_loader, test_loader = ut.load_dataset(dataset_name='fashion-mnist', minibatch=512)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(ut.train(train_loader, net, optimizer, criterion, 1))\n",
    "    print(ut.test(test_loader, net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "The first implementation uses the **whole training set**, in order to avoid noisy evaluations. \n",
    "The epsilon variable allows to set the maximum movement in every direction. Each move is done by creating a tensor for each layer $k$ called $ \\Delta_k(w)$. The update is done according to the formula:\n",
    "\n",
    "$$ w'  = w - \\epsilon \\Delta(w)  $$\n",
    "\n",
    "where each value of each tensor  $ \\Delta_k(w) $ is sampled from the distribution $ U (-1, 1) $. \n",
    "\n",
    "In the following implementation I wanted to try a first test without making the best choice, in order to save some computation time. If $ L(w') > L(w) $ the opposite direction is chosen, i.e.\n",
    "\n",
    "$$ w''  = w + \\epsilon \\ \\Delta(w)  $$\n",
    "\n",
    "even if $ L(w') < L(w'') $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def full_train_SA_noT(trainloader, model, accuracy_before, epsilon, gpu=True):\n",
    "    model.train()\n",
    "    \n",
    "    # List to store the opposite direction - \\Delta(w) for each layer\n",
    "    inverse = []\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        # Replicate the tensor\n",
    "        tensor_size = param.data.size()\n",
    "        move = torch.zeros(tensor_size)\n",
    "        # Send it to the GPU\n",
    "        if gpu:\n",
    "            move = move.cuda()\n",
    "        # Generate move\n",
    "        move = move.uniform_(-1, 1).mul(epsilon) * param.data\n",
    "        # Stepback is saved\n",
    "        inverse.append(move.mul(-2))\n",
    "        # Move the parameters\n",
    "        param.data.add_(move)\n",
    "    \n",
    "    # Evaluate the accuracy \n",
    "    new_accuracy = ut.test_train(train_loader, net)[1]\n",
    "\n",
    "    if new_accuracy < accuracy_before:\n",
    "        for k, param in enumerate(net.parameters()):\n",
    "            param.data.add_(inverse[k])\n",
    "\n",
    "        new_accuracy = ut.test_train(train_loader, net)[1]\n",
    "    \n",
    "    #print(\"New accuracy: \", new_accuracy)\n",
    "    return new_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results (first plot below), it can be seen that both  directions do not often improve the accuracy. \n",
    "\n",
    "Now we can implement the full algorithm. $ L(w') $ and $ L(w'') $ are both evaulated:\n",
    "\n",
    "$$ L_{best} = \\min(L(w'),\\ L(w'')) $$\n",
    "\n",
    "If $ L_{best} < L(w) $, then accept the move for sure; otherwise accept if $ x  > temperature $, where x is sampled from the uniform distribution $ U(0, 1) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train_SA(trainloader, model, initial_accuracy, epsilon, T, gpu=True):\n",
    "    model.train()\n",
    "    \n",
    "    # List used to keep the move to get back to the initial point\n",
    "    inverse = []\n",
    "    \n",
    "    # First move\n",
    "    for param in net.parameters():\n",
    "        # Replicate the tensor\n",
    "        tensor_size = param.data.size()\n",
    "        move = torch.zeros(tensor_size)\n",
    "        # Send it to the GPU\n",
    "        if gpu:\n",
    "            move = move.cuda()\n",
    "        # Generate move\n",
    "        move = move.uniform_(-1, 1).mul(epsilon) * param.data\n",
    "        # Stepback is saved\n",
    "        inverse.append(move.mul(-1)) \n",
    "        # Move the parameters\n",
    "        param.data.add_(move)\n",
    "    # Evaluate the accuracy \n",
    "    first_accuracy = ut.test_train(train_loader, net)[1]\n",
    "    #print(\"First move accuracy: \", first_accuracy)\n",
    "    \n",
    "    # Second move\n",
    "    for k, param in enumerate(net.parameters()):\n",
    "        param.data.add_(inverse[k].mul(2))\n",
    "        inverse[k] = inverse[k].mul(-1)\n",
    "        print(param.size())\n",
    "    second_accuracy = ut.test_train(train_loader, net)[1]\n",
    "    #print(\"Second move accuracy:\", second_accuracy)\n",
    "    \n",
    "    # Get back if the first accuracy is better\n",
    "    if first_accuracy > second_accuracy:\n",
    "        for k, param in enumerate(net.parameters()):\n",
    "            param.data.add_(inverse[k].mul(2))\n",
    "            inverse[k] = inverse[k].mul(-1)\n",
    "        new_accuracy = first_accuracy\n",
    "    else: new_accuracy = second_accuracy\n",
    "    \n",
    "    # Accept a worse solution according to temperature\n",
    "    if new_accuracy < initial_accuracy and rm.uniform(0, 1) > T:\n",
    "        for k, param in enumerate(net.parameters()):\n",
    "            param.data.add_(inverse[k])\n",
    "        new_accuracy = initial_accuracy\n",
    "        \n",
    "    del move, inverse\n",
    "    #print(\"Final accuracy:\", new_accuracy)\n",
    "    return new_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the first method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epsilon = 10e-3\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=4096)\n",
    "\n",
    "# Testing the first method\n",
    "starting_time = time.time()\n",
    "net = Net()\n",
    "clone = copy.deepcopy(net)\n",
    "net = net.cuda()\n",
    "accuracy = ut.test_train(train_loader, net)[1]\n",
    "training_set_measurements, validation_set_measurements, times = [], [], []\n",
    "\n",
    "# Train\n",
    "for epoch in range(epochs):\n",
    "    #print(\"Epoch: \", epoch)\n",
    "    accuracy = full_train_SA_noT(train_loader, net, accuracy, epsilon)\n",
    "    training_set_measurements.append(accuracy)\n",
    "    validation_set_measurements.append(ut.test(test_loader, net)[1])\n",
    "    times.append(time.time() - starting_time)\n",
    "    \n",
    "np.savez('no_temperature',\n",
    "            training_set_measurements=[x * 100 for x in training_set_measurements],\n",
    "            validation_set_measurements=[x * 100 for x in validation_set_measurements],\n",
    "            times=[x / 60 for x in times])\n",
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the second method, forcing a monotonic behaviour, by alwasy setting $ Temperature = 0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the second method\n",
    "net = clone.cuda()\n",
    "accuracy = ut.test_train(train_loader, net)[1]\n",
    "training_set_measurements, validation_set_measurements, times = [], [], []\n",
    "starting_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #print(\"Epoch: \", epoch)\n",
    "    accuracy = full_train_SA(train_loader, net, accuracy, epsilon, 0)\n",
    "    training_set_measurements.append(accuracy)\n",
    "    validation_set_measurements.append(ut.test(test_loader, net)[1])\n",
    "    times.append(time.time() - starting_time)\n",
    "    \n",
    "np.savez('T=0',\n",
    "            training_set_measurements=[x * 100 for x in training_set_measurements],\n",
    "            validation_set_measurements=[x * 100 for x in validation_set_measurements],\n",
    "            times=[x / 60 for x in times])\n",
    "\n",
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two plots: the first one with 'number of epochs' as x-axis, the second one comparing the accuracy vs computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_T = np.load('no_temperature.npz')\n",
    "T_0 = np.load('T=0.npz')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(np.arange(0, len(no_T['training_set_measurements'])), no_T['training_set_measurements'], '--', label='No T')\n",
    "ax.plot(np.arange(0, len(T_0['training_set_measurements'])), T_0['training_set_measurements'], '--', label='T = 0')\n",
    "\n",
    "plt.title('Comparison between the two implementations')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(no_T['times'], no_T['training_set_measurements'], '--', label='No T')\n",
    "ax.plot(T_0['times'], T_0['training_set_measurements'], '--', label='T = 0')\n",
    "\n",
    "plt.xlabel('Minutes elapsed')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some information about training and test acuracy in the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(np.arange(0, len(T_0['training_set_measurements'])), T_0['training_set_measurements'], '--', label='Training set')\n",
    "ax.plot(np.arange(0, len(T_0['validation_set_measurements'])), T_0['validation_set_measurements'], '--', label='Validation set')\n",
    "\n",
    "plt.title('Temperature = 0')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_minibatch(images, labels, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = labels.size(0)\n",
    "    test_loss = 0\n",
    "    \n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    test_loss += float(F.cross_entropy(outputs, labels).item())\n",
    "    \n",
    "    return test_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGA(trainloader, model, epsilon, T, gpu=True):\n",
    "    model.train()\n",
    "    \n",
    "    not_accepted_worse, accepted_worse = 0, 0\n",
    "    tot = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        tot += 1\n",
    "        inputs, labels = data\n",
    "        if gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        \n",
    "        initial_loss = test_minibatch(inputs, labels, model)[0]\n",
    "        print(\"Initial loss: \", initial_loss)\n",
    "        # List used to keep the move to get back to the initial point\n",
    "        inverse = []\n",
    "\n",
    "        # First move\n",
    "        for param in model.parameters():\n",
    "            # Replicate the tensor\n",
    "            tensor_size = param.data.size()\n",
    "            move = torch.zeros(tensor_size)\n",
    "            # Send it to the GPU\n",
    "            if gpu:\n",
    "                move = move.cuda()\n",
    "            # Generate move\n",
    "            move.normal_(std=epsilon)\n",
    "            # Stepback is saved\n",
    "            inverse.append(move.mul(-1)) \n",
    "            # Move the parameters\n",
    "            param.data.add_(move)\n",
    "        \n",
    "        # Evaluate the loss \n",
    "        first_loss = test_minibatch(inputs, labels, model)[0]\n",
    "        print(\"First loss: \", first_loss)\n",
    "        \n",
    "        # Second move\n",
    "        for k, param in enumerate(model.parameters()):\n",
    "            param.data.add_(inverse[k].mul(2))\n",
    "            inverse[k] = inverse[k].mul(-1)\n",
    "        second_loss = test_minibatch(inputs, labels, model)[0]\n",
    "        print(\"Second loss: \", second_loss)\n",
    "\n",
    "        # Get back if the first move is better\n",
    "        if first_loss < second_loss:\n",
    "            for k, param in enumerate(model.parameters()):\n",
    "                param.data.add_(inverse[k].mul(2))\n",
    "                inverse[k] = inverse[k].mul(-1)\n",
    "            new_loss = first_loss\n",
    "        else: new_loss = second_loss\n",
    "        \n",
    "        # Accept a worse solution according to temperature\n",
    "        if new_loss > initial_loss and math.exp(- (new_loss - initial_loss) / T) < rm.random():\n",
    "            not_accepted_worse += 1\n",
    "            for k, param in enumerate(model.parameters()):\n",
    "                param.data.add_(inverse[k])\n",
    "            new_loss = initial_loss\n",
    "        elif new_loss > initial_loss:\n",
    "            accepted_worse += 1\n",
    "        print(\"FINAL LOSS: \", test_minibatch(inputs, labels, model)[0])\n",
    "        del move, inverse\n",
    "        \n",
    "    return not_accepted_worse, accepted_worse, tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(train_loader, model, lr):\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?ANJDJSA\n",
      "Epoch:  0 , epsilon: 0.001\n",
      "Initial loss:  0.004496290348470211\n",
      "First loss:  0.004494197200983763\n",
      "Second loss:  0.0044983262196183205\n",
      "FINAL LOSS:  0.004494197200983763\n",
      "Initial loss:  0.004498067311942577\n",
      "First loss:  0.004499836824834347\n",
      "Second loss:  0.004496455192565918\n",
      "FINAL LOSS:  0.004496455192565918\n",
      "Initial loss:  0.004498594906181097\n",
      "First loss:  0.004499051719903946\n",
      "Second loss:  0.004498641937971115\n",
      "FINAL LOSS:  0.004498641937971115\n",
      "Initial loss:  0.004492824897170067\n",
      "First loss:  0.004493100102990866\n",
      "Second loss:  0.004492649342864752\n",
      "FINAL LOSS:  0.004492649342864752\n",
      "Initial loss:  0.004502125084400177\n",
      "First loss:  0.0044995457865297794\n",
      "Second loss:  0.004505142569541931\n",
      "FINAL LOSS:  0.004499546252191067\n",
      "Initial loss:  0.004491614643484354\n",
      "First loss:  0.004491531290113926\n",
      "Second loss:  0.004491439089179039\n",
      "FINAL LOSS:  0.004491439089179039\n",
      "Initial loss:  0.00447770394384861\n",
      "First loss:  0.004478149581700563\n",
      "Second loss:  0.004477676935493946\n",
      "FINAL LOSS:  0.004477676935493946\n",
      "Initial loss:  0.0045034983195364475\n",
      "First loss:  0.004502706229686737\n",
      "Second loss:  0.004504262935370207\n",
      "FINAL LOSS:  0.004502706229686737\n",
      "Initial loss:  0.004480025731027126\n",
      "First loss:  0.004480988252907991\n",
      "Second loss:  0.0044794450514018536\n",
      "FINAL LOSS:  0.0044794450514018536\n",
      "Initial loss:  0.004490810912102461\n",
      "First loss:  0.004489471670240164\n",
      "Second loss:  0.00449233315885067\n",
      "FINAL LOSS:  0.004489471670240164\n",
      "Initial loss:  0.0044843764044344425\n",
      "First loss:  0.004482364282011986\n",
      "Second loss:  0.004486508201807737\n",
      "FINAL LOSS:  0.004482364282011986\n",
      "Initial loss:  0.004477318841964006\n",
      "First loss:  0.004472540225833654\n",
      "Second loss:  0.00448264367878437\n",
      "FINAL LOSS:  0.004472540225833654\n",
      "Initial loss:  0.004472279455512762\n",
      "First loss:  0.004470483865588903\n",
      "Second loss:  0.00447445223107934\n",
      "FINAL LOSS:  0.004470483865588903\n",
      "Initial loss:  0.0044861482456326485\n",
      "First loss:  0.004486726131290197\n",
      "Second loss:  0.004486141260713339\n",
      "FINAL LOSS:  0.004486141260713339\n",
      "Initial loss:  0.004470860119909048\n",
      "First loss:  0.004471283406019211\n",
      "Second loss:  0.004470932763069868\n",
      "FINAL LOSS:  0.004470932763069868\n",
      "Initial loss:  0.004493852611631155\n",
      "First loss:  0.0044931466691195965\n",
      "Second loss:  0.004494653083384037\n",
      "FINAL LOSS:  0.0044931466691195965\n",
      "Initial loss:  0.004468891769647598\n",
      "First loss:  0.004466162994503975\n",
      "Second loss:  0.004471807740628719\n",
      "FINAL LOSS:  0.004466162994503975\n",
      "Initial loss:  0.00446661189198494\n",
      "First loss:  0.004466932732611895\n",
      "Second loss:  0.004466868005692959\n",
      "FINAL LOSS:  0.004466868005692959\n",
      "Initial loss:  0.004476850852370262\n",
      "First loss:  0.0044735693372786045\n",
      "Second loss:  0.004480024334043264\n",
      "FINAL LOSS:  0.0044735693372786045\n",
      "Initial loss:  0.004485692363232374\n",
      "First loss:  0.0044836197048425674\n",
      "Second loss:  0.0044880276545882225\n",
      "FINAL LOSS:  0.004483620170503855\n",
      "Initial loss:  0.004472318105399609\n",
      "First loss:  0.0044740126468241215\n",
      "Second loss:  0.004470838233828545\n",
      "FINAL LOSS:  0.004470838233828545\n",
      "Initial loss:  0.004484893288463354\n",
      "First loss:  0.00448428513482213\n",
      "Second loss:  0.00448605976998806\n",
      "FINAL LOSS:  0.00448428513482213\n",
      "Initial loss:  0.004475178197026253\n",
      "First loss:  0.004476713016629219\n",
      "Second loss:  0.004474175162613392\n",
      "FINAL LOSS:  0.004474175162613392\n",
      "Initial loss:  0.004487529397010803\n",
      "First loss:  0.004486766643822193\n",
      "Second loss:  0.004488482605665922\n",
      "FINAL LOSS:  0.004486766178160906\n",
      "Initial loss:  0.004478590562939644\n",
      "First loss:  0.0044767423532903194\n",
      "Second loss:  0.0044809505343437195\n",
      "FINAL LOSS:  0.0044767423532903194\n",
      "Initial loss:  0.004461440723389387\n",
      "First loss:  0.0044600944966077805\n",
      "Second loss:  0.004463084042072296\n",
      "FINAL LOSS:  0.0044600944966077805\n",
      "Initial loss:  0.0044662971049547195\n",
      "First loss:  0.004462684970349073\n",
      "Second loss:  0.004470289219170809\n",
      "FINAL LOSS:  0.004462684970349073\n",
      "Initial loss:  0.0044691734947264194\n",
      "First loss:  0.00447142543271184\n",
      "Second loss:  0.004467349033802748\n",
      "FINAL LOSS:  0.004467349033802748\n",
      "Initial loss:  0.004464443307369947\n",
      "First loss:  0.004464218392968178\n",
      "Second loss:  0.004465396981686354\n",
      "FINAL LOSS:  0.004464218392968178\n",
      "Initial loss:  0.004460122901946306\n",
      "First loss:  0.0044580320827662945\n",
      "Second loss:  0.004463080316781998\n",
      "FINAL LOSS:  0.0044580320827662945\n",
      "Initial loss:  0.0044626048766076565\n",
      "First loss:  0.004461654927581549\n",
      "Second loss:  0.0044640484265983105\n",
      "FINAL LOSS:  0.004461655393242836\n",
      "Initial loss:  0.004474082961678505\n",
      "First loss:  0.004473659209907055\n",
      "Second loss:  0.0044751837849617004\n",
      "FINAL LOSS:  0.004473659209907055\n",
      "Initial loss:  0.004462276119738817\n",
      "First loss:  0.0044607315212488174\n",
      "Second loss:  0.004464610014110804\n",
      "FINAL LOSS:  0.0044607315212488174\n",
      "Initial loss:  0.004464587662369013\n",
      "First loss:  0.004463065881282091\n",
      "Second loss:  0.0044669476337730885\n",
      "FINAL LOSS:  0.004463065881282091\n",
      "Initial loss:  0.004467514343559742\n",
      "First loss:  0.004467373248189688\n",
      "Second loss:  0.004467872437089682\n",
      "FINAL LOSS:  0.004467373248189688\n",
      "Initial loss:  0.004458904732018709\n",
      "First loss:  0.004460467491298914\n",
      "Second loss:  0.0044581652618944645\n",
      "FINAL LOSS:  0.0044581652618944645\n",
      "Initial loss:  0.004462212324142456\n",
      "First loss:  0.004460590425878763\n",
      "Second loss:  0.0044642179273068905\n",
      "FINAL LOSS:  0.004460590425878763\n",
      "Initial loss:  0.004462724085897207\n",
      "First loss:  0.00446173083037138\n",
      "Second loss:  0.0044638169929385185\n",
      "FINAL LOSS:  0.00446173083037138\n",
      "Initial loss:  0.0044638775289058685\n",
      "First loss:  0.004463448189198971\n",
      "Second loss:  0.004464727360755205\n",
      "FINAL LOSS:  0.004463448189198971\n",
      "Initial loss:  0.004461093805730343\n",
      "First loss:  0.0044609555043280125\n",
      "Second loss:  0.004461717326194048\n",
      "FINAL LOSS:  0.0044609555043280125\n",
      "Initial loss:  0.004465318750590086\n",
      "First loss:  0.004468023777008057\n",
      "Second loss:  0.004462198819965124\n",
      "FINAL LOSS:  0.004462198819965124\n",
      "Initial loss:  0.00446217181161046\n",
      "First loss:  0.0044596632942557335\n",
      "Second loss:  0.004464908502995968\n",
      "FINAL LOSS:  0.0044596632942557335\n",
      "Initial loss:  0.004461314994841814\n",
      "First loss:  0.004459748510271311\n",
      "Second loss:  0.004462788347154856\n",
      "FINAL LOSS:  0.004459748510271311\n",
      "Initial loss:  0.004440209362655878\n",
      "First loss:  0.004443429410457611\n",
      "Second loss:  0.0044375695288181305\n",
      "FINAL LOSS:  0.0044375695288181305\n",
      "Initial loss:  0.004450707230716944\n",
      "First loss:  0.00444873608648777\n",
      "Second loss:  0.0044526900164783\n",
      "FINAL LOSS:  0.00444873608648777\n",
      "Initial loss:  0.004470201674848795\n",
      "First loss:  0.004470605403184891\n",
      "Second loss:  0.00447011599317193\n",
      "FINAL LOSS:  0.00447011599317193\n",
      "Initial loss:  0.004453062079846859\n",
      "First loss:  0.0044537256471812725\n",
      "Second loss:  0.004452934022992849\n",
      "FINAL LOSS:  0.004452934022992849\n",
      "Initial loss:  0.004443501587957144\n",
      "First loss:  0.004443137440830469\n",
      "Second loss:  0.004445090889930725\n",
      "FINAL LOSS:  0.004443137440830469\n",
      "Initial loss:  0.004453085362911224\n",
      "First loss:  0.004457131028175354\n",
      "Second loss:  0.004449564032256603\n",
      "FINAL LOSS:  0.004449564032256603\n",
      "Initial loss:  0.00442409748211503\n",
      "First loss:  0.0044250646606087685\n",
      "Second loss:  0.004423706326633692\n",
      "FINAL LOSS:  0.004423706326633692\n",
      "Initial loss:  0.004425182472914457\n",
      "First loss:  0.004426184576004744\n",
      "Second loss:  0.00442501250654459\n",
      "FINAL LOSS:  0.00442501250654459\n",
      "Initial loss:  0.004462689161300659\n",
      "First loss:  0.004466845653951168\n",
      "Second loss:  0.0044586798176169395\n",
      "FINAL LOSS:  0.0044586798176169395\n",
      "Initial loss:  0.0044434224255383015\n",
      "First loss:  0.004443249199539423\n",
      "Second loss:  0.0044437916949391365\n",
      "FINAL LOSS:  0.004443249199539423\n",
      "Initial loss:  0.004447388928383589\n",
      "First loss:  0.004447381943464279\n",
      "Second loss:  0.004447815008461475\n",
      "FINAL LOSS:  0.0044473824091255665\n",
      "Initial loss:  0.0044487239792943\n",
      "First loss:  0.004447986371815205\n",
      "Second loss:  0.00445057637989521\n",
      "FINAL LOSS:  0.004447986371815205\n",
      "Initial loss:  0.004467973951250315\n",
      "First loss:  0.004468547645956278\n",
      "Second loss:  0.004467948339879513\n",
      "FINAL LOSS:  0.004467948339879513\n",
      "Initial loss:  0.0044477395713329315\n",
      "First loss:  0.0044463942758738995\n",
      "Second loss:  0.0044494024477899075\n",
      "FINAL LOSS:  0.0044463942758738995\n",
      "Initial loss:  0.00443590572103858\n",
      "First loss:  0.004440702497959137\n",
      "Second loss:  0.0044312854297459126\n",
      "FINAL LOSS:  0.0044312854297459126\n",
      "Initial loss:  0.004448365420103073\n",
      "First loss:  0.0044469828717410564\n",
      "Second loss:  0.0044502392411231995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS:  0.0044469828717410564\n",
      "Initial loss:  0.004415388684719801\n",
      "First loss:  0.004414076916873455\n",
      "Second loss:  0.004416964948177338\n",
      "FINAL LOSS:  0.004414076916873455\n",
      "Initial loss:  0.00442732498049736\n",
      "First loss:  0.004422468598932028\n",
      "Second loss:  0.00443268520757556\n",
      "FINAL LOSS:  0.004422468598932028\n",
      "Initial loss:  0.004429394844919443\n",
      "First loss:  0.004428380634635687\n",
      "Second loss:  0.00443120626732707\n",
      "FINAL LOSS:  0.004428380634635687\n",
      "Initial loss:  0.004446734208613634\n",
      "First loss:  0.004450239706784487\n",
      "Second loss:  0.0044434694573283195\n",
      "FINAL LOSS:  0.0044434694573283195\n",
      "Initial loss:  0.004436968360096216\n",
      "First loss:  0.004435248672962189\n",
      "Second loss:  0.004438901320099831\n",
      "FINAL LOSS:  0.004435248672962189\n",
      "Initial loss:  0.0044268094934523106\n",
      "First loss:  0.004433284047991037\n",
      "Second loss:  0.004420856479555368\n",
      "FINAL LOSS:  0.004420856479555368\n",
      "Initial loss:  0.0044222441501915455\n",
      "First loss:  0.004423605278134346\n",
      "Second loss:  0.004421385005116463\n",
      "FINAL LOSS:  0.004421385005116463\n",
      "Initial loss:  0.004428952932357788\n",
      "First loss:  0.004432010464370251\n",
      "Second loss:  0.004426519852131605\n",
      "FINAL LOSS:  0.004426519852131605\n",
      "Initial loss:  0.004449259955435991\n",
      "First loss:  0.0044464897364377975\n",
      "Second loss:  0.004452300723642111\n",
      "FINAL LOSS:  0.0044464897364377975\n",
      "Initial loss:  0.004421412479132414\n",
      "First loss:  0.004423779901117086\n",
      "Second loss:  0.0044196173548698425\n",
      "FINAL LOSS:  0.0044196173548698425\n",
      "Initial loss:  0.004425481427460909\n",
      "First loss:  0.004424725193530321\n",
      "Second loss:  0.004427021369338036\n",
      "FINAL LOSS:  0.004424725193530321\n",
      "Initial loss:  0.004432860761880875\n",
      "First loss:  0.004432705231010914\n",
      "Second loss:  0.0044334824196994305\n",
      "FINAL LOSS:  0.004432705231010914\n",
      "Initial loss:  0.0044153109192848206\n",
      "First loss:  0.004417285323143005\n",
      "Second loss:  0.004413741175085306\n",
      "FINAL LOSS:  0.004413741175085306\n",
      "Initial loss:  0.004415577743202448\n",
      "First loss:  0.004417781718075275\n",
      "Second loss:  0.004413721151649952\n",
      "FINAL LOSS:  0.004413721151649952\n",
      "Initial loss:  0.004442593082785606\n",
      "First loss:  0.004443290643393993\n",
      "Second loss:  0.004442361183464527\n",
      "FINAL LOSS:  0.004442361183464527\n",
      "Initial loss:  0.004428436979651451\n",
      "First loss:  0.004429559223353863\n",
      "Second loss:  0.004427642095834017\n",
      "FINAL LOSS:  0.004427642095834017\n",
      "Initial loss:  0.004404534585773945\n",
      "First loss:  0.004403355531394482\n",
      "Second loss:  0.0044059450738132\n",
      "FINAL LOSS:  0.004403355065733194\n",
      "Initial loss:  0.004454780835658312\n",
      "First loss:  0.004455978982150555\n",
      "Second loss:  0.0044534397311508656\n",
      "FINAL LOSS:  0.0044534397311508656\n",
      "Initial loss:  0.00442462507635355\n",
      "First loss:  0.004421482793986797\n",
      "Second loss:  0.00442793034017086\n",
      "FINAL LOSS:  0.004421482793986797\n",
      "Initial loss:  0.004410012625157833\n",
      "First loss:  0.004409200511872768\n",
      "Second loss:  0.004411159083247185\n",
      "FINAL LOSS:  0.004409200511872768\n",
      "Initial loss:  0.0044027091935276985\n",
      "First loss:  0.00440473947674036\n",
      "Second loss:  0.004401455633342266\n",
      "FINAL LOSS:  0.004401455633342266\n",
      "Initial loss:  0.0044013927690684795\n",
      "First loss:  0.0043984889052808285\n",
      "Second loss:  0.004405227955430746\n",
      "FINAL LOSS:  0.004398488439619541\n",
      "Initial loss:  0.004428699612617493\n",
      "First loss:  0.004427076317369938\n",
      "Second loss:  0.004430662374943495\n",
      "FINAL LOSS:  0.004427075851708651\n",
      "Initial loss:  0.004415006376802921\n",
      "First loss:  0.004418857395648956\n",
      "Second loss:  0.004411492962390184\n",
      "FINAL LOSS:  0.004411492962390184\n",
      "Initial loss:  0.00441388413310051\n",
      "First loss:  0.004413323942571878\n",
      "Second loss:  0.004414821043610573\n",
      "FINAL LOSS:  0.004413323942571878\n",
      "Initial loss:  0.004415955860167742\n",
      "First loss:  0.0044186534360051155\n",
      "Second loss:  0.004413503222167492\n",
      "FINAL LOSS:  0.004413503222167492\n",
      "Initial loss:  0.004399548750370741\n",
      "First loss:  0.004402422346174717\n",
      "Second loss:  0.0043968320824205875\n",
      "FINAL LOSS:  0.0043968320824205875\n",
      "Initial loss:  0.004396340809762478\n",
      "First loss:  0.0043999566696584225\n",
      "Second loss:  0.004393451381474733\n",
      "FINAL LOSS:  0.004393451381474733\n",
      "Initial loss:  0.004402998369187117\n",
      "First loss:  0.004401355050504208\n",
      "Second loss:  0.00440517021343112\n",
      "FINAL LOSS:  0.004401355050504208\n",
      "Initial loss:  0.004386901389807463\n",
      "First loss:  0.0043864683248102665\n",
      "Second loss:  0.004388134460896254\n",
      "FINAL LOSS:  0.0043864683248102665\n",
      "Initial loss:  0.004395314492285252\n",
      "First loss:  0.004392237402498722\n",
      "Second loss:  0.004399525001645088\n",
      "FINAL LOSS:  0.004392237402498722\n",
      "Initial loss:  0.004392640199512243\n",
      "First loss:  0.004391930066049099\n",
      "Second loss:  0.004393890034407377\n",
      "FINAL LOSS:  0.004391930066049099\n",
      "Initial loss:  0.004405139945447445\n",
      "First loss:  0.004407767206430435\n",
      "Second loss:  0.004402706865221262\n",
      "FINAL LOSS:  0.004402706865221262\n",
      "Initial loss:  0.004405824933201075\n",
      "First loss:  0.004404083359986544\n",
      "Second loss:  0.004408174194395542\n",
      "FINAL LOSS:  0.004404083359986544\n",
      "Initial loss:  0.004402806982398033\n",
      "First loss:  0.004404392093420029\n",
      "Second loss:  0.004401968792080879\n",
      "FINAL LOSS:  0.004401968792080879\n",
      "Initial loss:  0.004400762263685465\n",
      "First loss:  0.004402020946145058\n",
      "Second loss:  0.004400378093123436\n",
      "FINAL LOSS:  0.004400378093123436\n",
      "Initial loss:  0.004389715380966663\n",
      "First loss:  0.004393687006086111\n",
      "Second loss:  0.004386720713227987\n",
      "FINAL LOSS:  0.004386720713227987\n",
      "Initial loss:  0.004373888019472361\n",
      "First loss:  0.004372838418930769\n",
      "Second loss:  0.004375357646495104\n",
      "FINAL LOSS:  0.004372838418930769\n",
      "Initial loss:  0.004391533322632313\n",
      "First loss:  0.004395252093672752\n",
      "Second loss:  0.004388777073472738\n",
      "FINAL LOSS:  0.004388777073472738\n",
      "Initial loss:  0.004388739354908466\n",
      "First loss:  0.004391005262732506\n",
      "Second loss:  0.0043868557550013065\n",
      "FINAL LOSS:  0.0043868557550013065\n",
      "Initial loss:  0.004374258685857058\n",
      "First loss:  0.0043750666081905365\n",
      "Second loss:  0.004374162759631872\n",
      "FINAL LOSS:  0.004374162759631872\n",
      "Initial loss:  0.004379482939839363\n",
      "First loss:  0.004377479664981365\n",
      "Second loss:  0.004382295534014702\n",
      "FINAL LOSS:  0.004377479664981365\n",
      "Initial loss:  0.004375746473670006\n",
      "First loss:  0.004379694350063801\n",
      "Second loss:  0.0043729799799621105\n",
      "FINAL LOSS:  0.0043729799799621105\n",
      "Initial loss:  0.004366541281342506\n",
      "First loss:  0.004366642329841852\n",
      "Second loss:  0.0043668001890182495\n",
      "FINAL LOSS:  0.004366642329841852\n",
      "Initial loss:  0.0043714046478271484\n",
      "First loss:  0.004370172508060932\n",
      "Second loss:  0.00437321187928319\n",
      "FINAL LOSS:  0.004370172042399645\n",
      "Initial loss:  0.00437402306124568\n",
      "First loss:  0.004373424220830202\n",
      "Second loss:  0.0043747322633862495\n",
      "FINAL LOSS:  0.004373424220830202\n",
      "Initial loss:  0.004368881229311228\n",
      "First loss:  0.004374610260128975\n",
      "Second loss:  0.004363601561635733\n",
      "FINAL LOSS:  0.004363601561635733\n",
      "Initial loss:  0.004369684960693121\n",
      "First loss:  0.004368484485894442\n",
      "Second loss:  0.0043714907951653\n",
      "FINAL LOSS:  0.004368484485894442\n",
      "Initial loss:  0.004378870595246553\n",
      "First loss:  0.00437585124745965\n",
      "Second loss:  0.004382245242595673\n",
      "FINAL LOSS:  0.00437585124745965\n",
      "Initial loss:  0.004363141022622585\n",
      "First loss:  0.004361440427601337\n",
      "Second loss:  0.004365450702607632\n",
      "FINAL LOSS:  0.004361440427601337\n",
      "Initial loss:  0.004385918378829956\n",
      "First loss:  0.00438386807218194\n",
      "Second loss:  0.004388598725199699\n",
      "FINAL LOSS:  0.00438386807218194\n",
      "Initial loss:  0.0043706633150577545\n",
      "First loss:  0.0043702200055122375\n",
      "Second loss:  0.004371833521872759\n",
      "FINAL LOSS:  0.0043702200055122375\n",
      "Initial loss:  0.00435239914804697\n",
      "First loss:  0.004354862961918116\n",
      "Second loss:  0.004350568167865276\n",
      "FINAL LOSS:  0.004350568167865276\n",
      "Initial loss:  0.0043503460474312305\n",
      "First loss:  0.0043520210310816765\n",
      "Second loss:  0.004349094815552235\n",
      "FINAL LOSS:  0.004349094815552235\n",
      "Initial loss:  0.004366000648587942\n",
      "First loss:  0.004366384353488684\n",
      "Second loss:  0.004366052802652121\n",
      "FINAL LOSS:  0.004366052802652121\n",
      "Initial loss:  0.004365293309092522\n",
      "First loss:  0.004363585263490677\n",
      "Second loss:  0.004368424415588379\n",
      "FINAL LOSS:  0.00436358479782939\n",
      "Initial loss:  0.004365316592156887\n",
      "First loss:  0.004362411797046661\n",
      "Second loss:  0.004368399735540152\n",
      "FINAL LOSS:  0.004362411797046661\n",
      "Initial loss:  0.004386366810649633\n",
      "First loss:  0.004381638020277023\n",
      "Second loss:  0.004391602240502834\n",
      "FINAL LOSS:  0.004381638020277023\n",
      "Initial loss:  0.02324533959229787\n",
      "First loss:  0.02324409286181132\n",
      "Second loss:  0.02324609210093816\n",
      "FINAL LOSS:  0.02324409286181132\n",
      "Non accepted:  0\n",
      "Accepted:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: (0.0043962231834729516, 0.21848333333333333)\n",
      "Validation error: (0.004470748376846314, 0.2092)\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = ut.load_dataset(dataset_name='mnist', minibatch=512)\n",
    "\n",
    "# Parameters\n",
    "epsilon = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "# Testing the second method\n",
    "net = ConvNet().cuda()\n",
    "training_set_measurements, validation_set_measurements, times = [], [], []\n",
    "starting_time = time.time()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(0):\n",
    "    ut.train(train_loader, net, optimizer, criterion, 1)\n",
    "    print(\"Training error:\", ut.test_train(train_loader, net))\n",
    "    print(\"Validation error:\", ut.test(test_loader, net))\n",
    "T = 1\n",
    "print(\"?ANJDJSA\")\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: \", epoch, \", epsilon:\", epsilon)\n",
    "    \n",
    "    T = 0.97 * T\n",
    "    not_accepted_worse, accepted_worse, tot = SGA(train_loader, net, epsilon, T)\n",
    "    \n",
    "    if (1 - (not_accepted_worse + accepted_worse)) / tot < 0.7:\n",
    "        T = 1\n",
    "    print(\"Non accepted: \", not_accepted_worse)\n",
    "    print(\"Accepted: \", accepted_worse)\n",
    "    \n",
    "    training_set_measurements.append(ut.test_train_sample(train_loader, net))\n",
    "    validation_set_measurements.append(ut.test(test_loader, net))\n",
    "    \n",
    "    print(\"Training error:\", training_set_measurements[epoch])\n",
    "    print(\"Validation error:\", validation_set_measurements[epoch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    print(\"Epoch: \", epoch, \", epsilon:\", 1e-4)\n",
    "    \n",
    "    loss = SGA(train_loader, net, 1e-4, 0)\n",
    "    print(\"Training error:\", ut.test_train(train_loader, net))\n",
    "    print(\"Validation error:\", ut.test(test_loader, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "import src.utils as ut\n",
    "net = ut.load_net(net='vgg11', dataset_name='mnist').cuda()\n",
    "\n",
    "net.train()\n",
    "print(\"Len: \", len(list(net.parameters())))\n",
    "for param in net.parameters():\n",
    "  print(param.size())\n",
    "  \n",
    "del net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
